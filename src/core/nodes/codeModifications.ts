import { HumanMessage } from "@langchain/core/messages";
import type { ChatOllama } from "@langchain/ollama";
import type { AgentStateType } from "../../types/state";
import ModificationCodeChangesPrompt from "../../types/prompts/modify/CodeChanges";

export const createCodeNode = (
  coderModel: ChatOllama,
  updateOutput: (
    type: "log" | "markdown",
    content: string,
    title?: string
  ) => void,
  signal?: AbortSignal
) => {
  return async (state: AgentStateType) => {
    updateOutput(
      "log",
      "ðŸ’» [IMPLEMENTATION] Generating code changes (llama3.1:latest)..."
    );

    const prompt = ModificationCodeChangesPrompt(
      state.codeContext,
      state.thinkingProcess,
      state.query
    );

    const promptSize = prompt.length;
    updateOutput("log", `Coder prompt size: ${promptSize} characters`);

    let modifications = "";
    const stream = await coderModel.stream([new HumanMessage(prompt)], {
      signal,
    });
    for await (const chunk of stream) {
      const content = chunk.content as string;
      modifications += content;
      updateOutput("markdown", modifications, "Implementation Details");
    }

    updateOutput("log", "Implementation complete.");

    return {
      modifications,
      llmContext: {
        ...state.llmContext,
        coderPromptSize: promptSize,
      },
      status: "code_generated",
    };
  };
};
